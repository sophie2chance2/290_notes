Goals:
- Follow a conversation about Gen AI
- Ask meaningful questions
- Know how to find out more
- Read and understand current research papers

# Weekly News
![](photos/Pasted%20image%2020240503161000.png)
- NEW type of model: SLM - Small Language Model
	- Able to run it on all types of machines, much more accessible than something like ChatGPT (LLM) which takes a significant amount of hardware
	- These small models (in green) are outperforming some of the LLMs
	- Microsoft is claiming that this model will run on your iPhone - actually doing the computation on your phone

# Introduction to Generative AI
## Why are we even talking about Gen AI?
- In 2021 we weren't talking about it at all even though most of this existed
- A confluence of events:
	- Recent developments in NLP allow for generation of flawless language
		- Transformers and neural architectures
		- Self supervised training
		- Internet scale data - huge amounts of data compared to original models of 7 novels and wikipedia
	- Recent developments allow for generation of photorealistic images not conditioned by text input
		- Say in text what you want an image to be
	- Hundreds of millions of people started using chatGPT within 2 months for a wide variety of applications
	- New models are being released on a weekly basis. Many different companies producing these models
- Gen AI Models are Very Capable
	- ![](photos/Pasted%20image%2020240503162136.png)
	- Image generated by Gen AI - Jason Allen spend 80 hours working on Mid journey prompt 
- Not Capable
	- ![](photos/Pasted%20image%2020240503162403.png)
	- Models are continuing to be refined to avoid mistakes like this

## What do we mean by Generative?
- Generative
	- ![](photos/Pasted%20image%2020240503162905.png)
	- Using new tools and techniques:
		- Vector and Matrix representations
		- Transformers
		- Self-supervised learning techniques
		- Digital content for training
		- Hardware (GPU, TPU, etc.)
	- We can *generate* a wide variety of novel outputs having learned distributions
	- 'Generative' because the model is generating some novel output (conditioned on the input)
		- Input helps the mould the probability space
		- Output is generated one token at a time (for text)
		- ![](photos/Pasted%20image%2020240503163410.png)

## What is 'Artificial Intelligence'?
- Gen AI models product some amazing outputs
	- Appear to demonstrate 'intelligence'
- Artificial intelligence is what we label such behavior in computers
- Intelligence
	- No definition.....
	- Different diciplines have different ideas of the definition
		- ![](photos/Pasted%20image%2020240503164449.png)
	- Why are we calling these LLMs intelligent?
		- They seem to understand us
		- Respond rationally
		- follow directions
		- preform new tasks
		- generate novel outputs
	- Anthropomorphic Projection onto LLMs
	- Neural net consists of neurons like the brain
		- Clever Hans problem?
- We want a 'competent assistant'
	- Don't want preset answers

## Brief history of Generative AI
- Turing Test
- Chomsky
- Parts of speech - "Fruit flies like a banana"
- Block world - need to know how to move things accordingly
	- ![](photos/Pasted%20image%2020240503170241.png)
- ![](photos/Pasted%20image%2020240503170337.png)
- Cyc - Project to replicate the knowledge of a 7 year old
	- Did not know that new borns could not walk
- MYCIN (1975) - Narrowly focused on diagnosing bacterial infections
- AlexNet (2010) - Identified 1000 different images 
- Speech Recognition 
	- It's hard to recognize speech v It's hard to wreck a nice beach
	- Alexa, Siri, Google
- Self Driving Cars/ Robots
	- So many edge cases
	- Not quite ready, getting closer everyday
	- DARPA Grand Challenge (2004)

## Where are we now?
- Neural Nets
	- Complex phenomenon as Vectors and matrices
	- Multiple layers allow for learning complex decision boundaries
	- Simple non-linear classification
	- ![](photos/Pasted%20image%2020240503171033.png)
	- ![](photos/Pasted%20image%2020240503171204.png)
	- LLMs 
		- building on the translation
		- predict next token based on previous token sequence
		- Tokens can be words or other segments (musical note, etc.)
		- LLM really a device with these attributes
			- Transformer architecture
			- Segmentation/tokenization
			- pre-trained on some tasks
			- Manipulates embeddings to create some output
	- ELIZA
		- ![](photos/Pasted%20image%2020240503171628.png)
	- ![](photos/Pasted%20image%2020240503171823.png)
	- ![](photos/Pasted%20image%2020240503171953.png)

## Implications of Evaluation
- How should we evaluate GenAI? Same as humans?
- Evaluate in one dimension or many dimensions?
	- GPT-4 Tech Report Test Scores
	- HELM - Holistic evaluation of LMs
- ![](photos/Pasted%20image%2020240503172233.png)
- ![](photos/Pasted%20image%2020240503172410.png)
- MMMU: A new benchmark designed to evaluate multimodal models on massive multi-discipline tasks demanding college-level subject knowledge and deliberate reasoning
	- ![](photos/Pasted%20image%2020240503172749.png)
- Evaluation is a growing area of research
	- Many claims/tests
## Beyond Evaluation (safety and ethical issues)?
- ![](photos/Pasted%20image%2020240503173200.png)
- ![](photos/Pasted%20image%2020240503173232.png)
- ![](photos/Pasted%20image%2020240503173352.png)
- ![](photos/Pasted%20image%2020240503173449.png)
- Bias/Fairness
	- What was the model trained on?
	- What kinds of bias will it reflect?
- ![](photos/Pasted%20image%2020240503173534.png)
- ![](photos/Pasted%20image%2020240503173930.png)



